import { getBlob } from "./worker_blob_cache";

let cachedSession = {};

self.addEventListener("message", (event) => {
  const data = event.data;
  if (data.kind === "init") init(data);
  if (data.kind === "isAlive") isAlive(data.modelUrl);
  if (data.kind === "phonemize") init(data, true);
});

/**
 * Checks if the session is alive.
 *
 * Each model has its own session.
 *
 * @param {string} modelUrl - The URL of the voice model file.
 */
const isAlive = (modelUrl) => {
  self.postMessage({
    kind: "isAlive",
    isAlive: cachedSession[modelUrl] != null,
  });
};

/**
 * Initializes the Piper phonemizer and generates the phonemes.
 *
 * @param {Object} data - The data required for initialization and processing.
 * @param {string} data.input - The text input to be phonemized and converted to speech.
 * @param {number} data.speakerId - The ID of the speaker to be used for synthesis.
 * @param {Object} data.blobs - A dictionary of pre-fetched blobs.
 * @param {string} data.modelUrl - The URL of the model file.
 * @param {string} data.modelConfigUrl - The URL of the model configuration file.
 * @param {string} data.piperPhonemizeJsUrl - The URL of the Piper phonemize JavaScript file.
 * @param {string} data.piperPhonemizeWasmUrl - The URL of the Piper phonemize WASM file.
 * @param {string} data.piperPhonemizeDataUrl - The URL of the Piper phonemize data file.
 * @param {int[]?} data.phonemeIds - The phonemes to be used for synthesis (as generated by piperPhonemize).
 * @param {string} onnxruntimeBase - The base URL for the ONNX Runtime Web.
 * @param {Object} modelConfig - The model configuration object.
 *
 * @returns {Promise<int[]>} A promise that resolves when the initialization and processing is complete.
 */
async function phonemize(data, onnxruntimeBase, modelConfig) {
  const { input, speakerId, blobs, modelUrl, modelConfigUrl } = data;

  const piperPhonemizeJs = URL.createObjectURL(
    await getBlob(data.piperPhonemizeJsUrl, blobs)
  );
  const piperPhonemizeWasm = URL.createObjectURL(
    await getBlob(data.piperPhonemizeWasmUrl, blobs)
  );
  const piperPhonemizeData = URL.createObjectURL(
    await getBlob(data.piperPhonemizeDataUrl, blobs)
  );

  importScripts(piperPhonemizeJs);

  const phonemeIds = await new Promise(async (resolve) => {
    const module = await createPiperPhonemize({
      print: (data) => {
        resolve(JSON.parse(data).phoneme_ids);
      },
      printErr: (message) => {
        self.postMessage({ kind: "stderr", message });
      },
      locateFile: (url, _scriptDirectory) => {
        if (url.endsWith(".wasm")) return piperPhonemizeWasm;
        if (url.endsWith(".data")) return piperPhonemizeData;
        return url;
      },
    });

    module.callMain([
      "-l",
      modelConfig.espeak.voice,
      "--input",
      JSON.stringify([{ text: input }]),
      "--espeak_data",
      "/espeak-ng-data",
    ]);
  });
  return phonemeIds;
}

/**
 * Initializes the Piper phonemizer and generates audio output.
 *
 * @param {Object} data - The data required for initialization and processing.
 * @param {string} data.input - The text input to be phonemized and converted to speech.
 * @param {number} data.speakerId - The ID of the speaker to be used for synthesis.
 * @param {Object} data.blobs - A dictionary of pre-fetched blobs.
 * @param {string} data.modelUrl - The URL of the voice model file.
 * @param {string} data.modelConfigUrl - The URL of the voice model configuration file.
 * @param {string} data.piperPhonemizeJsUrl - The URL of the Piper phonemize JavaScript file.
 * @param {string} data.piperPhonemizeWasmUrl - The URL of the Piper phonemize WASM file.
 * @param {string} data.piperPhonemizeDataUrl - The URL of the Piper phonemize data file.
 * @param {boolean?} phonemizeOnly - Flag indicating if only phonemization is required.
 * @param {int[]?} data.phonemeIds - The phonemes to be used for synthesis (as generated by piperPhonemize). Will be generated if not provided.
 * @param {string} onnxruntimeUrl - The URL for the ONNX Runtime Web.
 *
 * @returns {Promise<void>} A promise that resolves when the initialization and processing is complete.
 */
async function init(data, phonemizeOnly = false) {
  const { input, speakerId, blobs, modelUrl, modelConfigUrl, onnxruntimeUrl } =
    data;
  const modelConfigBlob = await getBlob(modelConfigUrl, blobs);
  const modelConfig = JSON.parse(await modelConfigBlob.text());

  const onnxruntimeBase = onnxruntimeUrl;
  const providedPhonemeIds = data.phonemeIds;

  const phonemeIds =
    providedPhonemeIds ?? (await phonemize(data, onnxruntimeBase, modelConfig));

  const phonemeIdMap = Object.entries(modelConfig.phoneme_id_map);
  const idPhonemeMap = Object.fromEntries(
    phonemeIdMap.map(([k, v]) => [v[0], k])
  );
  const phonemes = phonemeIds.map((id) => idPhonemeMap[id]);
  if (phonemizeOnly) {
    self.postMessage({ kind: "output", input, phonemes, phonemeIds });
    self.postMessage({ kind: "complete" });
    return;
  }
  const onnxruntimeJs = URL.createObjectURL(
    await getBlob(`${onnxruntimeBase}ort.min.js`, blobs)
  );

  importScripts(onnxruntimeJs);
  ort.env.wasm.numThreads = navigator.hardwareConcurrency;
  ort.env.wasm.wasmPaths = onnxruntimeBase;

  const sampleRate = modelConfig.audio.sample_rate;
  const numChannels = 1;
  const noiseScale = modelConfig.inference.noise_scale;
  const lengthScale = modelConfig.inference.length_scale;
  const noiseW = modelConfig.inference.noise_w;

  const modelBlob = await getBlob(modelUrl, blobs);
  const session =
    cachedSession[modelUrl] ??
    (await ort.InferenceSession.create(URL.createObjectURL(modelBlob)));
  // reset session cache if modelUrl has changed
  if (Object.keys(cachedSession).length && !cachedSession[modelUrl])
    cachedSession = {};
  cachedSession[modelUrl] = session;

  const feeds = {
    input: new ort.Tensor("int64", phonemeIds, [1, phonemeIds.length]),
    input_lengths: new ort.Tensor("int64", [phonemeIds.length]),
    scales: new ort.Tensor("float32", [noiseScale, lengthScale, noiseW]),
  };
  if (Object.keys(modelConfig.speaker_id_map).length)
    feeds.sid = new ort.Tensor("int64", [speakerId]);
  const {
    output: { data: pcm },
  } = await session.run(feeds);

  // Float32Array (PCM) to ArrayBuffer (WAV)
  function PCM2WAV(buffer, sampleRate, numChannels) {
    const bufferLength = buffer.length;
    const headerLength = 44;
    const view = new DataView(
      new ArrayBuffer(bufferLength * numChannels * 2 + headerLength)
    );

    view.setUint32(0, 0x46464952, true); // "RIFF"
    view.setUint32(4, view.buffer.byteLength - 8, true); // RIFF size
    view.setUint32(8, 0x45564157, true); // "WAVE"

    view.setUint32(12, 0x20746d66, true); // Subchunk1ID ("fmt ")
    view.setUint32(16, 0x10, true); // Subchunk1Size
    view.setUint16(20, 0x0001, true); // AudioFormat
    view.setUint16(22, numChannels, true); // NumChannels
    view.setUint32(24, sampleRate, true); // SampleRate
    view.setUint32(28, numChannels * 2 * sampleRate, true); // ByteRate
    view.setUint16(32, numChannels * 2, true); // BlockAlign
    view.setUint16(34, 16, true); // BitsPerSample

    view.setUint32(36, 0x61746164, true); // Subchunk2ID ("data")
    view.setUint32(40, 2 * bufferLength, true); // Subchunk2Size

    let p = headerLength;
    for (let i = 0; i < bufferLength; i++) {
      const v = buffer[i];
      if (v >= 1) view.setInt16(p, 0x7fff, true);
      else if (v <= -1) view.setInt16(p, -0x8000, true);
      else view.setInt16(p, (v * 0x8000) | 0, true);
      p += 2;
    }
    const wavBuffer = view.buffer;
    const duration = bufferLength / (sampleRate * numChannels);

    return { wavBuffer, duration };
  }

  const result = PCM2WAV(pcm, sampleRate, numChannels);
  const file = new Blob([result.wavBuffer], { type: "audio/x-wav" });
  const duration = Math.floor(result.duration * 1000);
  self.postMessage({
    kind: "output",
    input,
    file,
    duration,
    phonemes,
    phonemeIds,
  });
  self.postMessage({ kind: "complete" });
}
